{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac8b2adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully\n",
      "üìä Ready for geotechnical data extraction\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: IMPORTS AND SETUP\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "import pdfplumber\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")\n",
    "print(\"üìä Ready for geotechnical data extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd35bdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enhanced GeotechnicalDataExtractor class defined with improved grain size extraction\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: ENHANCED GEOTECHNICAL DATA EXTRACTOR CLASS\n",
    "# =============================================================================\n",
    "\n",
    "class GeotechnicalDataExtractor:\n",
    "    \"\"\"\n",
    "    Enhanced Geotechnical Data Extraction System\n",
    "    Fixed version with improved grain size extraction patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_directory):\n",
    "        self.data_dir = Path(data_directory)\n",
    "        self.complete_reports = []\n",
    "        self.text_plots = []\n",
    "        \n",
    "    def identify_file_types(self):\n",
    "        \"\"\"Identify and categorize PDF files\"\"\"\n",
    "        all_files = list(self.data_dir.glob(\"*.pdf\"))\n",
    "        \n",
    "        print(f\"üîç Found {len(all_files)} PDF files\")\n",
    "        \n",
    "        for file in all_files:\n",
    "            filename = file.name.lower()\n",
    "            \n",
    "            # Extract project ID pattern\n",
    "            project_id_match = re.search(r'(\\d{4}\\s*-\\s*\\d{2})', filename)\n",
    "            if not project_id_match:\n",
    "                continue\n",
    "                \n",
    "            clean_id = project_id_match.group(1).replace(' ', '')\n",
    "            \n",
    "            if \"complete report\" in filename:\n",
    "                self.complete_reports.append({\n",
    "                    'project_id': clean_id,\n",
    "                    'file_path': file,\n",
    "                    'type': 'complete_report'\n",
    "                })\n",
    "                print(f\"  ‚úÖ Complete Report: {clean_id}\")\n",
    "                \n",
    "            elif \"text plot\" in filename:\n",
    "                self.text_plots.append({\n",
    "                    'project_id': clean_id,\n",
    "                    'file_path': file,\n",
    "                    'type': 'text_plot'\n",
    "                })\n",
    "                print(f\"  ‚úÖ Text Plot: {clean_id}\")\n",
    "        \n",
    "        print(f\"\\nüìä Summary: {len(self.complete_reports)} Complete Reports, {len(self.text_plots)} Text Plots\")\n",
    "        return self.complete_reports, self.text_plots\n",
    "\n",
    "    def extract_target_variables(self, text_plot_file):\n",
    "        \"\"\"Extract bearing capacity and foundation type from Text Plot\"\"\"\n",
    "        try:\n",
    "            with pdfplumber.open(text_plot_file) as pdf:\n",
    "                full_text = \"\"\n",
    "                for page in pdf.pages:\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        full_text += page_text + \"\\n\"\n",
    "            \n",
    "            # Enhanced bearing capacity patterns\n",
    "            bearing_patterns = [\n",
    "                r'(\\d+\\.?\\d*)\\s*Tonne/ft[¬≤2]',\n",
    "                r'(\\d+\\.?\\d*)\\s*T/ft[¬≤2]', \n",
    "                r'(\\d+\\.?\\d*)\\s*ton/ft[¬≤2]',\n",
    "                r'Net\\s*Allowable\\s*Bearing\\s*Capacity.*?(\\d+\\.?\\d*)',\n",
    "                r'Safe\\s*Bearing\\s*Capacity.*?(\\d+\\.?\\d*)',\n",
    "                r'Bearing\\s*Capacity.*?(\\d+\\.?\\d*)',\n",
    "                r'Allowable\\s*Bearing\\s*Pressure.*?(\\d+\\.?\\d*)',\n",
    "                r'qa\\s*=\\s*(\\d+\\.?\\d*)'\n",
    "            ]\n",
    "            \n",
    "            bearing_capacities = []\n",
    "            for pattern in bearing_patterns:\n",
    "                matches = re.findall(pattern, full_text, re.IGNORECASE)\n",
    "                for match in matches:\n",
    "                    try:\n",
    "                        value = float(match)\n",
    "                        # Validate bearing capacity (reasonable range: 0.1 - 50 T/ft¬≤)\n",
    "                        if 0.1 <= value <= 50:\n",
    "                            bearing_capacities.append(value)\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "            \n",
    "            # Foundation type detection\n",
    "            foundation_type = \"Unknown\"\n",
    "            text_upper = full_text.upper()\n",
    "            if \"RAFT FOUNDATION\" in text_upper or \"MAT FOUNDATION\" in text_upper:\n",
    "                foundation_type = \"Raft\"\n",
    "            elif \"PILE FOUNDATION\" in text_upper or \"DEEP FOUNDATION\" in text_upper:\n",
    "                foundation_type = \"Pile\"\n",
    "            elif \"SHALLOW FOUNDATION\" in text_upper or \"SPREAD FOOTING\" in text_upper:\n",
    "                foundation_type = \"Shallow\"\n",
    "            elif \"ISOLATED FOOTING\" in text_upper:\n",
    "                foundation_type = \"Isolated\"\n",
    "            \n",
    "            return {\n",
    "                'bearing_capacities': bearing_capacities,\n",
    "                'foundation_type': foundation_type\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error extracting targets: {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_comprehensive_lab_data(self, complete_report_file):\n",
    "        \"\"\"Extract all possible laboratory parameters using text patterns and tables\"\"\"\n",
    "        try:\n",
    "            with pdfplumber.open(complete_report_file) as pdf:\n",
    "                extracted_data = []\n",
    "                \n",
    "                for page_num, page in enumerate(pdf.pages):\n",
    "                    text = page.extract_text()\n",
    "                    tables = page.extract_tables()\n",
    "                    \n",
    "                    if text:\n",
    "                        # Text-based extraction with enhanced patterns\n",
    "                        text_data = self._extract_from_text_patterns_enhanced(text)\n",
    "                        extracted_data.extend(text_data)\n",
    "                    \n",
    "                    if tables:\n",
    "                        # Table-based extraction with enhanced mapping\n",
    "                        for table in tables:\n",
    "                            table_data = self._extract_from_table_enhanced(table)\n",
    "                            extracted_data.extend(table_data)\n",
    "                \n",
    "                return extracted_data\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error extracting lab data: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _extract_from_text_patterns_enhanced(self, text):\n",
    "        \"\"\"Enhanced extraction with comprehensive grain size patterns\"\"\"\n",
    "        extracted = []\n",
    "        \n",
    "        # Comprehensive parameter patterns\n",
    "        patterns = {\n",
    "            'moisture_content_pct': [\n",
    "                r'(?:moisture|water)\\s*content.*?(\\d+\\.?\\d*)\\s*%',\n",
    "                r'w\\s*=\\s*(\\d+\\.?\\d*)\\s*%',\n",
    "                r'M\\.C\\..*?(\\d+\\.?\\d*)\\s*%',\n",
    "                r'moisture.*?(\\d+\\.?\\d*)\\s*%'\n",
    "            ],\n",
    "            'liquid_limit_ll': [\n",
    "                r'liquid\\s*limit.*?(\\d+\\.?\\d*)\\s*%?',\n",
    "                r'LL\\s*=\\s*(\\d+\\.?\\d*)',\n",
    "                r'L\\.L\\..*?(\\d+\\.?\\d*)',\n",
    "                r'Liquid\\s*Limit\\s*[=:]\\s*(\\d+\\.?\\d*)'\n",
    "            ],\n",
    "            'plastic_limit_pl': [\n",
    "                r'plastic\\s*limit.*?(\\d+\\.?\\d*)\\s*%?',\n",
    "                r'PL\\s*=\\s*(\\d+\\.?\\d*)',\n",
    "                r'P\\.L\\..*?(\\d+\\.?\\d*)',\n",
    "                r'Plastic\\s*Limit\\s*[=:]\\s*(\\d+\\.?\\d*)'\n",
    "            ],\n",
    "            'sand_pct': [\n",
    "                # Basic sand patterns\n",
    "                r'sand\\s*[=:]\\s*(\\d+\\.?\\d*)\\s*%',\n",
    "                r'(\\d+\\.?\\d*)\\s*%\\s*sand',\n",
    "                r'SAND\\s*[=:]\\s*(\\d+\\.?\\d*)\\s*%',\n",
    "                r'Sand\\s*[=:]\\s*(\\d+\\.?\\d*)\\s*%',\n",
    "                \n",
    "                # Sand fraction patterns\n",
    "                r'(?:fine|medium|coarse)\\s*sand\\s*[=:]\\s*(\\d+\\.?\\d*)\\s*%',\n",
    "                r'(\\d+\\.?\\d*)\\s*%\\s*(?:fine|medium|coarse)\\s*sand',\n",
    "                r'sand\\s*fraction\\s*[=:]\\s*(\\d+\\.?\\d*)\\s*%',\n",
    "                r'total\\s*sand\\s*[=:]\\s*(\\d+\\.?\\d*)\\s*%',\n",
    "                \n",
    "                # Sieve-based patterns for sand sizes (0.075mm to 4.75mm)\n",
    "                r'retained\\s*(?:on\\s*)?(?:#\\s*)?(?:4|8|16|30|50|100)\\s*[^\\d]*(\\d+\\.?\\d*)\\s*%',\n",
    "                r'passing\\s*(?:#\\s*)?(?:4|8|16|30|50)\\s*[^\\d]*(\\d+\\.?\\d*)\\s*%',\n",
    "                r'#\\s*(?:4|8|16|30|50|100)\\s*[^\\d]*(\\d+\\.?\\d*)\\s*%',\n",
    "                \n",
    "                # Size range patterns\n",
    "                r'0\\.075.*?4\\.75.*?(\\d+\\.?\\d*)\\s*%',\n",
    "                r'(?:0\\.15|0\\.3|0\\.6|1\\.18|2\\.36)\\s*mm.*?(\\d+\\.?\\d*)\\s*%'\n",
    "            ],\n",
    "            'gravel_pct': [\n",
    "                # Basic gravel patterns\n",
    "                r'gravel\\s*[=:]\\s*(\\d+\\.?\\d*)\\s*%',\n",
    "                r'(\\d+\\.?\\d*)\\s*%\\s*gravel',\n",
    "                r'GRAVEL\\s*[=:]\\s*(\\d+\\.?\\d*)\\s*%',\n",
    "                r'Gravel\\s*[=:]\\s*(\\d+\\.?\\d*)\\s*%',\n",
    "                \n",
    "                # Gravel fraction patterns\n",
    "                r'(?:fine|coarse)\\s*gravel\\s*[=:]\\s*(\\d+\\.?\\d*)\\s*%',\n",
    "                r'(\\d+\\.?\\d*)\\s*%\\s*(?:fine|coarse)\\s*gravel',\n",
    "                r'gravel\\s*fraction\\s*[=:]\\s*(\\d+\\.?\\d*)\\s*%',\n",
    "                r'total\\s*gravel\\s*[=:]\\s*(\\d+\\.?\\d*)\\s*%',\n",
    "                \n",
    "                # Sieve-based patterns for gravel sizes (>4.75mm)\n",
    "                r'retained\\s*(?:on\\s*)?(?:#\\s*)?(?:4|3/8|1/2|3/4|1)\\s*[^\\d]*(\\d+\\.?\\d*)\\s*%',\n",
    "                r'#\\s*4\\s*[^\\d]*(\\d+\\.?\\d*)\\s*%',\n",
    "                \n",
    "                # Size range patterns for gravel\n",
    "                r'(?:4\\.75|9\\.5|12\\.5|19|25)\\s*mm.*?(\\d+\\.?\\d*)\\s*%',\n",
    "                r'(?:3/16|3/8|1/2|3/4|1)[\\\"\\s]*.*?(\\d+\\.?\\d*)\\s*%'\n",
    "            ],\n",
    "            'fines_pct': [\n",
    "                # Basic fines patterns\n",
    "                r'fines?\\s*[=:]\\s*(\\d+\\.?\\d*)\\s*%',\n",
    "                r'(\\d+\\.?\\d*)\\s*%\\s*fines?',\n",
    "                r'FINES?\\s*[=:]\\s*(\\d+\\.?\\d*)\\s*%',\n",
    "                \n",
    "                # Sieve #200 patterns (standard for fines)\n",
    "                r'passing\\s*(?:#\\s*)?200\\s*[^\\d]*(\\d+\\.?\\d*)\\s*%',\n",
    "                r'#\\s*200\\s*[^\\d]*(\\d+\\.?\\d*)\\s*%',\n",
    "                r'(?:sieve\\s*)?200\\s*[^\\d]*(\\d+\\.?\\d*)\\s*%',\n",
    "                r'200\\s*mesh.*?(\\d+\\.?\\d*)\\s*%',\n",
    "                \n",
    "                # Combined silt+clay\n",
    "                r'silt\\s*[+&]\\s*clay\\s*[=:]\\s*(\\d+\\.?\\d*)\\s*%',\n",
    "                r'(?:silt|clay)\\s*fraction\\s*[=:]\\s*(\\d+\\.?\\d*)\\s*%',\n",
    "                r'fine\\s*fraction\\s*[=:]\\s*(\\d+\\.?\\d*)\\s*%',\n",
    "                \n",
    "                # Size-based (fines are <0.075mm)\n",
    "                r'<\\s*0\\.075\\s*mm.*?(\\d+\\.?\\d*)\\s*%',\n",
    "                r'0\\.075\\s*mm\\s*passing.*?(\\d+\\.?\\d*)\\s*%'\n",
    "            ],\n",
    "            'bulk_density': [\n",
    "                r'bulk\\s*density.*?(\\d+\\.?\\d*)',\n",
    "                r'dry\\s*density.*?(\\d+\\.?\\d*)',\n",
    "                r'Œ≥d.*?(\\d+\\.?\\d*)',\n",
    "                r'unit\\s*weight.*?(\\d+\\.?\\d*)'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Validation ranges for each parameter\n",
    "        validation_ranges = {\n",
    "            'moisture_content_pct': (0, 100),\n",
    "            'liquid_limit_ll': (0, 200),\n",
    "            'plastic_limit_pl': (0, 100),\n",
    "            'sand_pct': (0, 100),\n",
    "            'gravel_pct': (0, 100),\n",
    "            'fines_pct': (0, 100),\n",
    "            'bulk_density': (0.5, 3.0)\n",
    "        }\n",
    "        \n",
    "        for param, pattern_list in patterns.items():\n",
    "            valid_values = []\n",
    "            \n",
    "            for pattern in pattern_list:\n",
    "                matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)\n",
    "                for match in matches:\n",
    "                    try:\n",
    "                        value = float(match)\n",
    "                        \n",
    "                        # Apply validation\n",
    "                        if param in validation_ranges:\n",
    "                            min_val, max_val = validation_ranges[param]\n",
    "                            if min_val <= value <= max_val:\n",
    "                                valid_values.append(value)\n",
    "                                \n",
    "                                # Debug output for grain size parameters\n",
    "                                if param in ['sand_pct', 'gravel_pct'] and value > 0:\n",
    "                                    print(f\"  üéØ Found {param}: {value}% using pattern\")\n",
    "                        else:\n",
    "                            valid_values.append(value)\n",
    "                            \n",
    "                    except ValueError:\n",
    "                        continue\n",
    "            \n",
    "            if valid_values:\n",
    "                # Use median to avoid outliers\n",
    "                final_value = sorted(valid_values)[len(valid_values)//2]\n",
    "                extracted.append({\n",
    "                    'parameter': param,\n",
    "                    'value': final_value,\n",
    "                    'source': 'enhanced_text'\n",
    "                })\n",
    "        \n",
    "        return extracted\n",
    "\n",
    "    def _extract_from_table_enhanced(self, table):\n",
    "        \"\"\"Enhanced table extraction with better grain size mapping\"\"\"\n",
    "        if not table or len(table) < 2:\n",
    "            return []\n",
    "        \n",
    "        extracted = []\n",
    "        headers = table[0] if table[0] else []\n",
    "        \n",
    "        # Enhanced header mapping for grain size analysis\n",
    "        header_mapping = {\n",
    "            'depth': ['depth', 'dep', 'd', 'elevation'],\n",
    "            'moisture_content_pct': ['moisture', 'water content', 'w%', 'mc', 'w.c.'],\n",
    "            'liquid_limit_ll': ['liquid limit', 'll', 'l.l', 'liquid'],\n",
    "            'plastic_limit_pl': ['plastic limit', 'pl', 'p.l', 'plastic'],\n",
    "            'sand_pct': [\n",
    "                'sand', 'sand%', 'sand %', '% sand', 'total sand',\n",
    "                'fine sand', 'medium sand', 'coarse sand', 'sand fraction',\n",
    "                '#4', '#8', '#16', '#30', '#50', '#100', 'retained #4'\n",
    "            ],\n",
    "            'gravel_pct': [\n",
    "                'gravel', 'gravel%', 'gravel %', '% gravel', 'total gravel',\n",
    "                'fine gravel', 'coarse gravel', 'gravel fraction',\n",
    "                'retained #4', '3/8', '1/2', '3/4', '1 inch'\n",
    "            ],\n",
    "            'fines_pct': [\n",
    "                'fines', 'fine', 'fines%', 'fines %', '% fines',\n",
    "                'passing 200', '#200', 'sieve 200', '200 mesh',\n",
    "                'silt + clay', 'silt+clay', 'clay+silt', 'fine fraction'\n",
    "            ],\n",
    "            'bulk_density': ['bulk density', 'density', 'Œ≥d', 'unit weight', 'dry density'],\n",
    "            'uscs_classification': ['uscs', 'classification', 'class', 'soil type'],\n",
    "            'spt_n_value': ['n value', 'n-value', 'spt', 'n', 'blows'],\n",
    "            'borehole_no': ['borehole', 'bh', 'bore', 'hole']\n",
    "        }\n",
    "        \n",
    "        # Map headers to parameters\n",
    "        column_mapping = {}\n",
    "        for i, header in enumerate(headers):\n",
    "            if header:\n",
    "                header_lower = str(header).lower().strip()\n",
    "                \n",
    "                for param, keywords in header_mapping.items():\n",
    "                    if any(keyword in header_lower for keyword in keywords):\n",
    "                        column_mapping[i] = param\n",
    "                        print(f\"  üìã Mapped column {i} '{header}' to {param}\")\n",
    "                        break\n",
    "        \n",
    "        # Extract data from rows\n",
    "        for row_idx, row in enumerate(table[1:]):\n",
    "            if not row:\n",
    "                continue\n",
    "            \n",
    "            for col_idx, cell_value in enumerate(row):\n",
    "                if col_idx in column_mapping and cell_value:\n",
    "                    param = column_mapping[col_idx]\n",
    "                    \n",
    "                    if param == 'uscs_classification':\n",
    "                        uscs_match = re.search(r'\\b(CL|CH|ML|MH|SM|SC|SW|SP|GW|GP|GM|GC)\\b', \n",
    "                                             str(cell_value).upper())\n",
    "                        if uscs_match:\n",
    "                            extracted.append({\n",
    "                                'parameter': param,\n",
    "                                'value': uscs_match.group(1),\n",
    "                                'source': 'enhanced_table'\n",
    "                            })\n",
    "                    else:\n",
    "                        # Extract numeric values with validation\n",
    "                        numeric_value = None\n",
    "                        \n",
    "                        if isinstance(cell_value, str):\n",
    "                            # Clean the string and extract number\n",
    "                            clean_cell = re.sub(r'[^\\d\\.]', '', str(cell_value))\n",
    "                            if clean_cell:\n",
    "                                try:\n",
    "                                    numeric_value = float(clean_cell)\n",
    "                                except ValueError:\n",
    "                                    continue\n",
    "                        elif isinstance(cell_value, (int, float)):\n",
    "                            numeric_value = float(cell_value)\n",
    "                        \n",
    "                        if numeric_value is not None:\n",
    "                            # Apply validation for percentages\n",
    "                            if 'pct' in param and not (0 <= numeric_value <= 100):\n",
    "                                continue\n",
    "                            \n",
    "                            extracted.append({\n",
    "                                'parameter': param,\n",
    "                                'value': numeric_value,\n",
    "                                'source': 'enhanced_table'\n",
    "                            })\n",
    "                            \n",
    "                            # Debug output for grain size\n",
    "                            if param in ['sand_pct', 'gravel_pct'] and numeric_value > 0:\n",
    "                                print(f\"  üéØ Table extracted {param}: {numeric_value}%\")\n",
    "        \n",
    "        return extracted\n",
    "\n",
    "    def extract_soil_descriptions(self, complete_report_file):\n",
    "        \"\"\"Extract and parse soil descriptions\"\"\"\n",
    "        try:\n",
    "            with pdfplumber.open(complete_report_file) as pdf:\n",
    "                descriptions = []\n",
    "                \n",
    "                for page in pdf.pages:\n",
    "                    text = page.extract_text()\n",
    "                    if not text:\n",
    "                        continue\n",
    "                    \n",
    "                    # Enhanced soil description patterns\n",
    "                    patterns = [\n",
    "                        r'(\\d+\\.?\\d*)\\s*[-\\']\\s*(\\d+\\.?\\d*)\\s*[\\'\"]?\\s*:?\\s*([^\\n]+(?:clay|sand|silt|gravel)[^\\n]*)',\n",
    "                        r'(CL|CH|ML|MH|SM|SC|SW|SP|GW|GP|GM|GC)\\s*[:-]?\\s*([^\\n]+)',\n",
    "                        r'(brown|gray|grey|black|white|yellow|red|orange)\\s*([^\\n]*(?:clay|sand|silt|gravel)[^\\n]*)'\n",
    "                    ]\n",
    "                    \n",
    "                    for pattern in patterns:\n",
    "                        matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)\n",
    "                        for match in matches:\n",
    "                            parsed = self._parse_soil_description(match)\n",
    "                            if parsed:\n",
    "                                descriptions.append(parsed)\n",
    "                \n",
    "                return descriptions\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error extracting soil descriptions: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _parse_soil_description(self, match):\n",
    "        \"\"\"Parse soil description into structured features\"\"\"\n",
    "        if isinstance(match, tuple):\n",
    "            text = ' '.join(str(m) for m in match)\n",
    "        else:\n",
    "            text = str(match)\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        parsed = {}\n",
    "        \n",
    "        # Extract features\n",
    "        features = {\n",
    "            'soil_color': ['brown', 'gray', 'grey', 'black', 'white', 'yellow', 'red', 'orange'],\n",
    "            'consistency': ['soft', 'firm', 'stiff', 'hard', 'loose', 'dense'],\n",
    "            'moisture': ['dry', 'moist', 'wet', 'saturated'],\n",
    "            'primary_soil_type': ['clay', 'sand', 'silt', 'gravel'],\n",
    "            'uscs_classification': ['CL', 'CH', 'ML', 'MH', 'SM', 'SC', 'SW', 'SP', 'GW', 'GP', 'GM', 'GC']\n",
    "        }\n",
    "        \n",
    "        for feature, values in features.items():\n",
    "            for value in values:\n",
    "                if value.lower() in text_lower:\n",
    "                    parsed[feature] = value\n",
    "                    break\n",
    "        \n",
    "        # Extract depths\n",
    "        depth_match = re.search(r'(\\d+\\.?\\d*)\\s*[-\\']\\s*(\\d+\\.?\\d*)', text)\n",
    "        if depth_match:\n",
    "            parsed['depth_start'] = float(depth_match.group(1))\n",
    "            parsed['depth_end'] = float(depth_match.group(2))\n",
    "        \n",
    "        # Extract SPT values\n",
    "        spt_match = re.search(r'N\\s*=\\s*(\\d+)', text, re.IGNORECASE)\n",
    "        if spt_match:\n",
    "            parsed['spt_n_value'] = int(spt_match.group(1))\n",
    "        \n",
    "        return parsed if len(parsed) > 0 else None\n",
    "\n",
    "    def extract_spt_data(self, complete_report_file):\n",
    "        \"\"\"Extract SPT N-values\"\"\"\n",
    "        try:\n",
    "            with pdfplumber.open(complete_report_file) as pdf:\n",
    "                spt_values = []\n",
    "                \n",
    "                patterns = [\n",
    "                    r'N\\s*=\\s*(\\d+)',\n",
    "                    r'SPT\\s*(\\d+)',\n",
    "                    r'N-value\\s*(\\d+)',\n",
    "                    r'blow.*?(\\d+)'\n",
    "                ]\n",
    "                \n",
    "                for page in pdf.pages:\n",
    "                    text = page.extract_text()\n",
    "                    if not text:\n",
    "                        continue\n",
    "                    \n",
    "                    for pattern in patterns:\n",
    "                        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "                        for match in matches:\n",
    "                            try:\n",
    "                                value = int(match)\n",
    "                                # Validate SPT values (reasonable range: 0-100)\n",
    "                                if 0 <= value <= 100:\n",
    "                                    spt_values.append(value)\n",
    "                            except ValueError:\n",
    "                                continue\n",
    "                \n",
    "                return spt_values\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error extracting SPT data: {e}\")\n",
    "            return []\n",
    "\n",
    "    def create_comprehensive_dataset(self):\n",
    "        \"\"\"Create comprehensive dataset with all extracted features\"\"\"\n",
    "        print(f\"\\nüîÑ Processing {len(self.complete_reports)} projects...\")\n",
    "        all_data = []\n",
    "        \n",
    "        for complete_report in self.complete_reports:\n",
    "            project_id = complete_report['project_id']\n",
    "            complete_file = complete_report['file_path']\n",
    "            \n",
    "            # Find corresponding text plot\n",
    "            text_plot = next((tp for tp in self.text_plots if tp['project_id'] == project_id), None)\n",
    "            \n",
    "            print(f\"\\nüìÅ Processing Project {project_id}\")\n",
    "            \n",
    "            # Extract all data types\n",
    "            target_data = None\n",
    "            if text_plot:\n",
    "                target_data = self.extract_target_variables(text_plot['file_path'])\n",
    "                print(f\"  ‚úÖ Target variables extracted\")\n",
    "            else:\n",
    "                print(f\"  ‚ö†Ô∏è  No text plot found - no target variables\")\n",
    "            \n",
    "            lab_data = self.extract_comprehensive_lab_data(complete_file)\n",
    "            soil_data = self.extract_soil_descriptions(complete_file)\n",
    "            spt_data = self.extract_spt_data(complete_file)\n",
    "            \n",
    "            print(f\"  üìä Extracted: {len(lab_data)} lab params, {len(soil_data)} soil features, {len(spt_data)} SPT values\")\n",
    "            \n",
    "            all_data.append({\n",
    "                'project_id': project_id,\n",
    "                'target_data': target_data,\n",
    "                'lab_data': lab_data,\n",
    "                'soil_data': soil_data,\n",
    "                'spt_data': spt_data\n",
    "            })\n",
    "        \n",
    "        # Structure into DataFrame\n",
    "        dataset = self._structure_dataset(all_data)\n",
    "        print(f\"\\n‚úÖ Dataset created: {dataset.shape[0]} rows √ó {dataset.shape[1]} columns\")\n",
    "        \n",
    "        return dataset\n",
    "\n",
    "    def _structure_dataset(self, all_data):\n",
    "        \"\"\"Structure dataset with proper bearing capacity validation\"\"\"\n",
    "        rows = []\n",
    "        \n",
    "        # Get list of projects that actually have Text Plot files\n",
    "        text_plot_projects = {tp['project_id'] for tp in self.text_plots}\n",
    "        \n",
    "        for project in all_data:\n",
    "            project_id = project['project_id']\n",
    "            row = {'project_id': project_id}\n",
    "            \n",
    "            # STRICT bearing capacity validation - only from Text Plots\n",
    "            if project_id in text_plot_projects and project['target_data']:\n",
    "                if project['target_data']['bearing_capacities']:\n",
    "                    row['bearing_capacity'] = float(project['target_data']['bearing_capacities'][0])\n",
    "                else:\n",
    "                    row['bearing_capacity'] = None\n",
    "                row['foundation_type'] = project['target_data']['foundation_type']\n",
    "            else:\n",
    "                # No Text Plot = No bearing capacity\n",
    "                row['bearing_capacity'] = None\n",
    "                row['foundation_type'] = None\n",
    "            \n",
    "            # Process lab data\n",
    "            if project['lab_data']:\n",
    "                lab_params = {}\n",
    "                for entry in project['lab_data']:\n",
    "                    param = entry['parameter']\n",
    "                    value = entry['value']\n",
    "                    \n",
    "                    if param not in lab_params:\n",
    "                        lab_params[param] = []\n",
    "                    lab_params[param].append(value)\n",
    "                \n",
    "                # Average multiple values for numeric parameters\n",
    "                for param, values in lab_params.items():\n",
    "                    if values and isinstance(values[0], (int, float)):\n",
    "                        row[param] = sum(values) / len(values)\n",
    "                    elif values:\n",
    "                        row[param] = values[0]\n",
    "            \n",
    "            # Process soil data\n",
    "            if project['soil_data']:\n",
    "                soil_features = {}\n",
    "                for desc in project['soil_data']:\n",
    "                    for key, value in desc.items():\n",
    "                        if isinstance(value, (int, float)):\n",
    "                            if key not in soil_features:\n",
    "                                soil_features[key] = []\n",
    "                            soil_features[key].append(value)\n",
    "                        elif isinstance(value, str) and key not in soil_features:\n",
    "                            soil_features[key] = value\n",
    "                \n",
    "                for feature, values in soil_features.items():\n",
    "                    if isinstance(values, list) and values:\n",
    "                        if isinstance(values[0], (int, float)):\n",
    "                            row[feature] = sum(values) / len(values)\n",
    "                        else:\n",
    "                            row[feature] = values[0]\n",
    "                    elif isinstance(values, str):\n",
    "                        row[feature] = values\n",
    "            \n",
    "            # Process SPT data\n",
    "            if project['spt_data']:\n",
    "                row['avg_n_value'] = sum(project['spt_data']) / len(project['spt_data'])\n",
    "                row['max_n_value'] = max(project['spt_data'])\n",
    "                row['min_n_value'] = min(project['spt_data'])\n",
    "                row['n_value_count'] = len(project['spt_data'])\n",
    "            \n",
    "            rows.append(row)\n",
    "        \n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    def clean_dataset(self, df):\n",
    "        \"\"\"Clean and validate dataset\"\"\"\n",
    "        print(f\"\\nüßπ Cleaning dataset...\")\n",
    "        print(f\"Initial shape: {df.shape}\")\n",
    "        \n",
    "        # Fill missing numeric values with median\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            if df[col].isnull().any():\n",
    "                median_val = df[col].median()\n",
    "                df[col].fillna(median_val, inplace=True)\n",
    "                print(f\"  üìù Filled {col} missing values with median: {median_val:.2f}\")\n",
    "        \n",
    "        # Fill missing categorical values\n",
    "        categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "        for col in categorical_cols:\n",
    "            if df[col].isnull().any():\n",
    "                mode_val = df[col].mode().iloc[0] if not df[col].mode().empty else 'Unknown'\n",
    "                df[col].fillna(mode_val, inplace=True)\n",
    "                print(f\"  üìù Filled {col} missing values with: {mode_val}\")\n",
    "        \n",
    "        print(f\"‚úÖ Cleaned dataset shape: {df.shape}\")\n",
    "        return df\n",
    "\n",
    "    def generate_summary(self, df):\n",
    "        \"\"\"Generate comprehensive dataset summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üìä DATASET SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"\\nüìà Basic Statistics:\")\n",
    "        print(f\"   Projects: {len(df)}\")\n",
    "        print(f\"   Features: {len(df.columns)}\")\n",
    "        print(f\"   Complete projects (with bearing capacity): {df['bearing_capacity'].notna().sum() if 'bearing_capacity' in df.columns else 0}\")\n",
    "        \n",
    "        print(f\"\\nüéØ Target Variable (Bearing Capacity):\")\n",
    "        if 'bearing_capacity' in df.columns and df['bearing_capacity'].notna().any():\n",
    "            bc_stats = df['bearing_capacity'].describe()\n",
    "            print(f\"   Mean: {bc_stats['mean']:.2f} T/ft¬≤\")\n",
    "            print(f\"   Range: {bc_stats['min']:.2f} - {bc_stats['max']:.2f} T/ft¬≤\")\n",
    "            print(f\"   Std Dev: {bc_stats['std']:.2f}\")\n",
    "        \n",
    "        print(f\"\\nüîç Grain Size Analysis:\")\n",
    "        grain_cols = ['sand_pct', 'gravel_pct', 'fines_pct']\n",
    "        for col in grain_cols:\n",
    "            if col in df.columns:\n",
    "                non_null = df[col].notna().sum()\n",
    "                if non_null > 0:\n",
    "                    values = df[col].dropna()\n",
    "                    print(f\"   ‚úÖ {col:<12}: {non_null}/4 projects, Range: {values.min():.1f}% - {values.max():.1f}%\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå {col:<12}: No data extracted\")\n",
    "        \n",
    "        print(f\"\\nüìã Feature Completeness:\")\n",
    "        for col in df.columns:\n",
    "            non_null = df[col].notna().sum()\n",
    "            total = len(df)\n",
    "            coverage = (non_null/total)*100\n",
    "            status = \"‚úÖ\" if coverage == 100 else \"‚ö†Ô∏è\" if coverage >= 50 else \"‚ùå\"\n",
    "            print(f\"   {status} {col:<25}: {non_null}/{total} ({coverage:.0f}%)\")\n",
    "        \n",
    "        print(f\"\\nüèóÔ∏è Foundation Types:\")\n",
    "        if 'foundation_type' in df.columns:\n",
    "            foundation_counts = df['foundation_type'].value_counts()\n",
    "            for ftype, count in foundation_counts.items():\n",
    "                print(f\"   {ftype}: {count}\")\n",
    "        \n",
    "        return df.describe()\n",
    "\n",
    "    def save_dataset(self, df, filename='geotechnical_dataset_final.csv'):\n",
    "        \"\"\"Save dataset to CSV\"\"\"\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"\\nüíæ Dataset saved as: {filename}\")\n",
    "        print(f\"   Shape: {df.shape}\")\n",
    "        print(f\"   Columns: {list(df.columns)}\")\n",
    "\n",
    "print(\"‚úÖ Enhanced GeotechnicalDataExtractor class defined with improved grain size extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d104ef2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enhanced main execution function defined\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: MAIN EXECUTION FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function for geotechnical data extraction\"\"\"\n",
    "    \n",
    "    print(\"üöÄ Starting Enhanced Geotechnical Data Extraction System\")\n",
    "    print(\"=\"*65)\n",
    "    \n",
    "    # Initialize extractor\n",
    "    extractor = GeotechnicalDataExtractor(\"Data\")\n",
    "    \n",
    "    # Step 1: Identify files\n",
    "    print(\"\\nüìÅ Step 1: File Identification\")\n",
    "    complete_reports, text_plots = extractor.identify_file_types()\n",
    "    \n",
    "    if not complete_reports:\n",
    "        print(\"‚ùå No complete reports found!\")\n",
    "        return None, None\n",
    "    \n",
    "    # Step 2: Extract comprehensive dataset\n",
    "    print(\"\\nüîç Step 2: Enhanced Data Extraction\")\n",
    "    dataset = extractor.create_comprehensive_dataset()\n",
    "    \n",
    "    if dataset.empty:\n",
    "        print(\"‚ùå No data extracted!\")\n",
    "        return None, None\n",
    "    \n",
    "    # Step 3: Clean dataset\n",
    "    print(\"\\nüßπ Step 3: Data Cleaning\")\n",
    "    cleaned_dataset = extractor.clean_dataset(dataset)\n",
    "    \n",
    "    # Step 4: Generate summary\n",
    "    print(\"\\nüìä Step 4: Analysis & Summary\")\n",
    "    summary_stats = extractor.generate_summary(cleaned_dataset)\n",
    "    \n",
    "    # Step 5: Save results\n",
    "    print(\"\\nüíæ Step 5: Save Results\")\n",
    "    extractor.save_dataset(cleaned_dataset, 'geotechnical_dataset_enhanced_final.csv')\n",
    "    \n",
    "    print(\"\\n‚úÖ Enhanced Extraction Complete!\")\n",
    "    print(f\"Final dataset: {cleaned_dataset.shape[0]} projects √ó {cleaned_dataset.shape[1]} features\")\n",
    "    \n",
    "    return cleaned_dataset, summary_stats\n",
    "\n",
    "print(\"‚úÖ Enhanced main execution function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7700b97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ EXECUTING ENHANCED GEOTECHNICAL DATA EXTRACTION PIPELINE\n",
      "===========================================================================\n",
      "üöÄ Starting Enhanced Geotechnical Data Extraction System\n",
      "=================================================================\n",
      "\n",
      "üìÅ Step 1: File Identification\n",
      "üîç Found 6 PDF files\n",
      "  ‚úÖ Complete Report: 7144-25\n",
      "  ‚úÖ Complete Report: 7145-25\n",
      "  ‚úÖ Complete Report: 7155-25\n",
      "  ‚úÖ Text Plot: 7155-25\n",
      "  ‚úÖ Complete Report: 7157-25\n",
      "  ‚úÖ Text Plot: 7157-25\n",
      "\n",
      "üìä Summary: 4 Complete Reports, 2 Text Plots\n",
      "\n",
      "üîç Step 2: Enhanced Data Extraction\n",
      "\n",
      "üîÑ Processing 4 projects...\n",
      "\n",
      "üìÅ Processing Project 7144-25\n",
      "  ‚ö†Ô∏è  No text plot found - no target variables\n",
      "  üìã Mapped column 1 'AAAUUUGGG‚Äô‚Äô‚Äô 222000222555\n",
      "ation Report\n",
      "OOOVVVEEERRRSSSEEEAAASSS\n",
      "BBBLLLOOOCCCKKK' to liquid_limit_ll\n",
      "  üìã Mapped column 0 'Depth\n",
      "(ft)' to depth\n",
      "  üìã Mapped column 1 'BH ‚Äì 1' to borehole_no\n",
      "  üìã Mapped column 3 'BH - 2' to borehole_no\n",
      "  üìã Mapped column 0 'Depth' to depth\n",
      "  üéØ Found gravel_pct: 5.0% using pattern\n",
      "  üìã Mapped column 1 'Depth of Footing from' to depth\n",
      "  üìã Mapped column 3 'Reduced Level' to depth\n",
      "  üìã Mapped column 4 'Net Allowable\n",
      "Bearing Capacity' to liquid_limit_ll\n",
      "  üìã Mapped column 0 'Net Allowable' to liquid_limit_ll\n",
      "  üìã Mapped column 4 'No. 4 Sieve\n",
      "(0.425 mm)' to spt_n_value\n",
      "  üìã Mapped column 5 'No. 200 Sieve\n",
      "(0.075 mm)' to spt_n_value\n",
      "  üìã Mapped column 0 'No. 4 Sieve' to spt_n_value\n",
      "  üìã Mapped column 0 'No. 200 Sieve' to spt_n_value\n",
      "  üìã Mapped column 0 'Seismic\n",
      "Zone' to spt_n_value\n",
      "  üìã Mapped column 2 'Zone Factor\n",
      "‚ÄòZ‚Äô' to spt_n_value\n",
      "  üìã Mapped column 0 'Zone Factor' to spt_n_value\n",
      "  üìã Mapped column 0 'Client:\n",
      "MR. REHMAN UDDIN' to depth\n",
      "  üìã Mapped column 0 'Client:\n",
      "MR. REHMAN UDDIN' to depth\n",
      "  üìã Mapped column 0 'BOREHOLE' to borehole_no\n",
      "  üìã Mapped column 2 'BH - 1' to borehole_no\n",
      "  üìã Mapped column 4 'BH - 2' to borehole_no\n",
      "  üìã Mapped column 0 'SWIS-TECH\n",
      "ENGINEERS & CONTRACTORS\n",
      "RAWALPINDI' to depth\n",
      "  üìã Mapped column 4 'GRAIN SIZE DISTRIBUTION' to depth\n",
      "  üìã Mapped column 11 'PLOT NO. C - 52, STREET NO. 33A\n",
      "OVERSEAS BLOCK, PARK VIEW CITY\n",
      "ISLAMABAD' to depth\n",
      "  üìã Mapped column 0 'SWIS-TECH\n",
      "ENGINEERS & CONTRACTORS\n",
      "RAWALPINDI' to depth\n",
      "  üìã Mapped column 4 'GRAIN SIZE DISTRIBUTION' to depth\n",
      "  üìã Mapped column 11 'PLOT NO. C - 52, STREET NO. 33A\n",
      "OVERSEAS BLOCK, PARK VIEW CITY\n",
      "ISLAMABAD' to depth\n",
      "  üìã Mapped column 0 'SWIS-TECH\n",
      "ENGINEERS & CONTRACTORS\n",
      "RAWALPINDI' to depth\n",
      "  üìã Mapped column 4 'GRAIN SIZE DISTRIBUTION' to depth\n",
      "  üìã Mapped column 11 'PLOT NO. C - 52, STREET NO. 33A\n",
      "OVERSEAS BLOCK, PARK VIEW CITY\n",
      "ISLAMABAD' to depth\n",
      "  üìä Extracted: 44 lab params, 163 soil features, 16 SPT values\n",
      "\n",
      "üìÅ Processing Project 7145-25\n",
      "  ‚ö†Ô∏è  No text plot found - no target variables\n",
      "  üìã Mapped column 1 'AAAUUUGGG‚Äô‚Äô‚Äô 222000222555\n",
      "ation Report\n",
      "OOOVVVEEERRRSSSEEEAAASSS\n",
      "BBBLLLOOOCCCKKK' to liquid_limit_ll\n",
      "  üìã Mapped column 0 'Depth\n",
      "(ft)' to depth\n",
      "  üìã Mapped column 1 'BH ‚Äì 1' to borehole_no\n",
      "  üìã Mapped column 3 'BH - 2' to borehole_no\n",
      "  üìã Mapped column 0 'Depth' to depth\n",
      "  üéØ Found gravel_pct: 5.0% using pattern\n",
      "  üìã Mapped column 1 'Depth of Footing from' to depth\n",
      "  üìã Mapped column 3 'Reduced Level' to depth\n",
      "  üìã Mapped column 4 'Net Allowable\n",
      "Bearing Capacity' to liquid_limit_ll\n",
      "  üìã Mapped column 0 'Net Allowable' to liquid_limit_ll\n",
      "  üìã Mapped column 4 'No. 4 Sieve\n",
      "(0.425 mm)' to spt_n_value\n",
      "  üìã Mapped column 5 'No. 200 Sieve\n",
      "(0.075 mm)' to spt_n_value\n",
      "  üìã Mapped column 0 'No. 4 Sieve' to spt_n_value\n",
      "  üìã Mapped column 0 'No. 200 Sieve' to spt_n_value\n",
      "  üìã Mapped column 0 'Seismic\n",
      "Zone' to spt_n_value\n",
      "  üìã Mapped column 2 'Zone Factor\n",
      "‚ÄòZ‚Äô' to spt_n_value\n",
      "  üìã Mapped column 0 'Zone Factor' to spt_n_value\n",
      "  üìã Mapped column 0 'Client:\n",
      "MRS. MARIA NAVEED' to depth\n",
      "  üìã Mapped column 0 'Client:\n",
      "MRS. MARIA NAVEED' to depth\n",
      "  üìã Mapped column 0 'BOREHOLE' to borehole_no\n",
      "  üìã Mapped column 2 'BH - 1' to borehole_no\n",
      "  üìã Mapped column 4 'BH - 2' to borehole_no\n",
      "  üìã Mapped column 0 'SWIS-TECH\n",
      "ENGINEERS & CONTRACTORS\n",
      "RAWALPINDI' to depth\n",
      "  üìã Mapped column 4 'GRAIN SIZE DISTRIBUTION' to depth\n",
      "  üìã Mapped column 11 'PLOT NO. C - 53, STREET NO. 33A\n",
      "OVERSEAS BLOCK, PARK VIEW CITY\n",
      "ISLAMABAD' to depth\n",
      "  üìã Mapped column 0 'SWIS-TECH\n",
      "ENGINEERS & CONTRACTORS\n",
      "RAWALPINDI' to depth\n",
      "  üìã Mapped column 4 'GRAIN SIZE DISTRIBUTION' to depth\n",
      "  üìã Mapped column 11 'PLOT NO. C - 53, STREET NO. 33A\n",
      "OVERSEAS BLOCK, PARK VIEW CITY\n",
      "ISLAMABAD' to depth\n",
      "  üìã Mapped column 0 'SWIS-TECH\n",
      "ENGINEERS & CONTRACTORS\n",
      "RAWALPINDI' to depth\n",
      "  üìã Mapped column 4 'GRAIN SIZE DISTRIBUTION' to depth\n",
      "  üìã Mapped column 11 'PLOT NO. C - 53, STREET NO. 33A\n",
      "OVERSEAS BLOCK, PARK VIEW CITY\n",
      "ISLAMABAD' to depth\n",
      "  üìä Extracted: 34 lab params, 161 soil features, 13 SPT values\n",
      "\n",
      "üìÅ Processing Project 7155-25\n",
      "  ‚úÖ Target variables extracted\n",
      "  üìã Mapped column 1 'AAAUUUGGG‚Äô‚Äô‚Äô 222000222555\n",
      "ation Report' to spt_n_value\n",
      "  üìã Mapped column 0 'Depth\n",
      "(ft)' to depth\n",
      "  üìã Mapped column 1 'BH ‚Äì 1' to borehole_no\n",
      "  üìã Mapped column 3 'BH - 2' to borehole_no\n",
      "  üìã Mapped column 0 'Depth' to depth\n",
      "  üìã Mapped column 0 'Depth\n",
      "(ft)' to depth\n",
      "  üìã Mapped column 1 'BH ‚Äì 3' to borehole_no\n",
      "  üìã Mapped column 3 'BH - 4' to borehole_no\n",
      "  üìã Mapped column 0 'Depth' to depth\n",
      "  üìã Mapped column 1 'Benchmark fixed' to depth\n",
      "  üìã Mapped column 3 'Borehole Location' to spt_n_value\n",
      "  üìã Mapped column 4 'Elevation' to depth\n",
      "  üìã Mapped column 0 'Depth of Footing from\n",
      "the Lowest Existing\n",
      "Ground Level (-16.00 ft)' to depth\n",
      "  üìã Mapped column 1 'Reduced Level' to depth\n",
      "  üìã Mapped column 2 'Net Allowable\n",
      "Bearing Capacity' to liquid_limit_ll\n",
      "  üìã Mapped column 0 'Depth of Footing from' to depth\n",
      "  üìã Mapped column 0 'Net Allowable' to liquid_limit_ll\n",
      "  üìã Mapped column 0 'Seismic\n",
      "Zone' to spt_n_value\n",
      "  üìã Mapped column 2 'Zone Factor\n",
      "‚ÄòZ‚Äô' to spt_n_value\n",
      "  üìã Mapped column 0 'Zone Factor' to spt_n_value\n",
      "  üìã Mapped column 0 '40'-0\"\n",
      "BH: 1\n",
      "NSL: 1845.0\n",
      "Fill: Nil\n",
      "\"0-'031\n",
      "\"6-'89\n",
      "GATE' to liquid_limit_ll\n",
      "  üìã Mapped column 0 'Client:\n",
      "MR. EAZAZ DAR' to depth\n",
      "  üìã Mapped column 0 'Client:\n",
      "MR. EAZAZ DAR' to depth\n",
      "  üìã Mapped column 0 'Client:\n",
      "MR. EAZAZ DAR' to depth\n",
      "  üìã Mapped column 0 'Client:\n",
      "MR. EAZAZ DAR' to depth\n",
      "  üìã Mapped column 0 'BOREHOLE' to borehole_no\n",
      "  üìã Mapped column 2 'BH - 1' to borehole_no\n",
      "  üìã Mapped column 3 'BH - 2' to borehole_no\n",
      "  üìã Mapped column 6 'BH - 3' to borehole_no\n",
      "  üìã Mapped column 8 'BH - 4' to borehole_no\n",
      "  üìã Mapped column 0 'SWIS-TECH\n",
      "ENGINEERS & CONTRACTORS\n",
      "RAWALPINDI' to depth\n",
      "  üìã Mapped column 4 'GRAIN SIZE DISTRIBUTION' to depth\n",
      "  üìã Mapped column 11 'PLOT NO. 345-C, STREET NO. 12\n",
      "SECTOR-C, PN FARMS\n",
      "SIMLI DAM ROAD, ISLAMABAD' to depth\n",
      "  üìã Mapped column 0 'SWIS-TECH\n",
      "ENGINEERS & CONTRACTORS\n",
      "RAWALPINDI' to depth\n",
      "  üìã Mapped column 4 'GRAIN SIZE DISTRIBUTION' to depth\n",
      "  üìã Mapped column 11 'PLOT NO. 345-C, STREET NO. 12\n",
      "SECTOR-C, PN FARMS\n",
      "SIMLI DAM ROAD, ISLAMABAD' to depth\n",
      "  üìã Mapped column 0 'SWIS-TECH\n",
      "ENGINEERS & CONTRACTORS\n",
      "RAWALPINDI' to depth\n",
      "  üìã Mapped column 4 'GRAIN SIZE DISTRIBUTION' to depth\n",
      "  üìã Mapped column 11 'PLOT NO. 345-C, STREET NO. 12\n",
      "SECTOR-C, PN FARMS\n",
      "SIMLI DAM ROAD, ISLAMABAD' to depth\n",
      "  üìã Mapped column 0 'SWIS-TECH\n",
      "ENGINEERS & CONTRACTORS\n",
      "RAWALPINDI' to depth\n",
      "  üìã Mapped column 4 'GRAIN SIZE DISTRIBUTION' to depth\n",
      "  üìã Mapped column 11 'PLOT NO. 345-C, STREET NO. 12\n",
      "SECTOR-C, PN FARMS\n",
      "SIMLI DAM ROAD, ISLAMABAD' to depth\n",
      "  üìã Mapped column 0 'SWIS-TECH\n",
      "ENGINEERS & CONTRACTORS\n",
      "RAWALPINDI' to depth\n",
      "  üìã Mapped column 4 'GRAIN SIZE DISTRIBUTION' to depth\n",
      "  üìã Mapped column 11 'PLOT NO. 345-C, STREET NO. 12\n",
      "SECTOR-C, PN FARMS\n",
      "SIMLI DAM ROAD, ISLAMABAD' to depth\n",
      "  üìã Mapped column 0 'SWIS-TECH\n",
      "ENGINEERS & CONTRACTORS\n",
      "RAWALPINDI' to depth\n",
      "  üìã Mapped column 4 'GRAIN SIZE DISTRIBUTION' to depth\n",
      "  üìã Mapped column 11 'PLOT NO. 345-C, STREET NO. 12\n",
      "SECTOR-C, PN FARMS\n",
      "SIMLI DAM ROAD, ISLAMABAD' to depth\n",
      "  üìã Mapped column 0 'SWIS-TECH\n",
      "ENGINEERS & CONTRACTORS\n",
      "RAWALPINDI' to depth\n",
      "  üìã Mapped column 4 'GRAIN SIZE DISTRIBUTION' to depth\n",
      "  üìã Mapped column 11 'PLOT NO. 345-C, STREET NO. 12\n",
      "SECTOR-C, PN FARMS\n",
      "SIMLI DAM ROAD, ISLAMABAD' to depth\n",
      "  üìã Mapped column 0 '%\n",
      "STRAIN' to spt_n_value\n",
      "  üéØ Found gravel_pct: 0.1% using pattern\n",
      "  üìã Mapped column 0 '%\n",
      "STRAIN' to spt_n_value\n",
      "  üìã Mapped column 0 '%\n",
      "STRAIN' to spt_n_value\n",
      "  üìä Extracted: 167 lab params, 232 soil features, 31 SPT values\n",
      "\n",
      "üìÅ Processing Project 7157-25\n",
      "  ‚úÖ Target variables extracted\n",
      "  üìã Mapped column 0 'Subsoil Investig' to spt_n_value\n",
      "  üìã Mapped column 2 'ation Report' to spt_n_value\n",
      "  üìã Mapped column 0 'Depth\n",
      "(ft)' to depth\n",
      "  üìã Mapped column 1 'BH ‚Äì 1' to borehole_no\n",
      "  üìã Mapped column 3 'BH ‚Äì 2' to borehole_no\n",
      "  üìã Mapped column 0 'Depth' to depth\n",
      "  üéØ Found gravel_pct: 5.0% using pattern\n",
      "  üìã Mapped column 1 'Depth of Footing from' to depth\n",
      "  üìã Mapped column 3 'Reduced Level' to depth\n",
      "  üìã Mapped column 4 'Net Allowable\n",
      "Bearing Capacity' to liquid_limit_ll\n",
      "  üìã Mapped column 0 'Net Allowable' to liquid_limit_ll\n",
      "  üìã Mapped column 4 'No. 4 Sieve\n",
      "(0.425 mm)' to spt_n_value\n",
      "  üìã Mapped column 5 'No. 200 Sieve\n",
      "(0.075 mm)' to spt_n_value\n",
      "  üìã Mapped column 0 'No. 4 Sieve' to spt_n_value\n",
      "  üìã Mapped column 0 'No. 200 Sieve' to spt_n_value\n",
      "  üìã Mapped column 0 'Seismic\n",
      "Zone' to spt_n_value\n",
      "  üìã Mapped column 2 'Zone Factor\n",
      "‚ÄòZ‚Äô' to spt_n_value\n",
      "  üìã Mapped column 0 'Zone Factor' to spt_n_value\n",
      "  üìã Mapped column 0 'Client:\n",
      "MR. SYED DILSHAD ALAM' to depth\n",
      "  üìã Mapped column 0 'Client:\n",
      "MR. SYED DILSHAD ALAM' to depth\n",
      "  üìã Mapped column 0 'BOREHOLE NO.' to spt_n_value\n",
      "  üìã Mapped column 2 'BH ‚Äì 1' to borehole_no\n",
      "  üìã Mapped column 7 'BH ‚Äì 2' to borehole_no\n",
      "  üìã Mapped column 0 'SWIS-TECH\n",
      "ENGINEERS & CONTRACTORS\n",
      "RAWALPINDI' to depth\n",
      "  üìã Mapped column 4 'GRAIN SIZE DISTRIBUTION' to depth\n",
      "  üìã Mapped column 11 'PLOT NO. 223-A, STREET NO. 15\n",
      "BLOCK - B, FAISAL TOWN\n",
      "RAWALPINDI' to depth\n",
      "  üìã Mapped column 0 'SWIS-TECH\n",
      "ENGINEERS & CONTRACTORS\n",
      "RAWALPINDI' to depth\n",
      "  üìã Mapped column 4 'GRAIN SIZE DISTRIBUTION' to depth\n",
      "  üìã Mapped column 11 'PLOT NO. 223-A, STREET NO. 15\n",
      "BLOCK - B, FAISAL TOWN\n",
      "RAWALPINDI' to depth\n",
      "  üìã Mapped column 0 'SWIS-TECH\n",
      "ENGINEERS & CONTRACTORS\n",
      "RAWALPINDI' to depth\n",
      "  üìã Mapped column 4 'GRAIN SIZE DISTRIBUTION' to depth\n",
      "  üìã Mapped column 11 'PLOT NO. 223-A, STREET NO. 15\n",
      "BLOCK - B, FAISAL TOWN\n",
      "RAWALPINDI' to depth\n",
      "  üìã Mapped column 0 'SWIS-TECH\n",
      "ENGINEERS & CONTRACTORS\n",
      "RAWALPINDI' to depth\n",
      "  üìã Mapped column 4 'GRAIN SIZE DISTRIBUTION' to depth\n",
      "  üìã Mapped column 11 'PLOT NO. 223-A, STREET NO. 15\n",
      "BLOCK - B, FAISAL TOWN\n",
      "RAWALPINDI' to depth\n",
      "  üìã Mapped column 0 'SWIS-TECH\n",
      "ENGINEERS & CONTRACTORS\n",
      "RAWALPINDI' to depth\n",
      "  üìã Mapped column 4 'GRAIN SIZE DISTRIBUTION' to depth\n",
      "  üìã Mapped column 11 'PLOT NO. 223-A, STREET NO. 15\n",
      "BLOCK - B, FAISAL TOWN\n",
      "RAWALPINDI' to depth\n",
      "  üìã Mapped column 0 'SWIS-TECH\n",
      "ENGINEERS & CONTRACTORS\n",
      "RAWALPINDI' to depth\n",
      "  üìã Mapped column 4 'GRAIN SIZE DISTRIBUTION' to depth\n",
      "  üìã Mapped column 11 'PLOT NO. 223-A, STREET NO. 15\n",
      "BLOCK - B, FAISAL TOWN\n",
      "RAWALPINDI' to depth\n",
      "  üéØ Found gravel_pct: 7.3% using pattern\n",
      "  üìã Mapped column 0 '%\n",
      "STRAIN' to spt_n_value\n",
      "  üéØ Found gravel_pct: 9.0% using pattern\n",
      "  üìã Mapped column 0 '%\n",
      "STRAIN' to spt_n_value\n",
      "  üéØ Found gravel_pct: 3.5% using pattern\n",
      "  üìã Mapped column 0 '%\n",
      "STRAIN' to spt_n_value\n",
      "  üéØ Found gravel_pct: 7.3% using pattern\n",
      "  üìã Mapped column 0 '%\n",
      "STRAIN' to spt_n_value\n",
      "  üéØ Found gravel_pct: 4.2% using pattern\n",
      "  üìã Mapped column 0 '%\n",
      "STRAIN' to spt_n_value\n",
      "  üìä Extracted: 243 lab params, 227 soil features, 35 SPT values\n",
      "\n",
      "‚úÖ Dataset created: 4 rows √ó 22 columns\n",
      "\n",
      "üßπ Step 3: Data Cleaning\n",
      "\n",
      "üßπ Cleaning dataset...\n",
      "Initial shape: (4, 22)\n",
      "  üìù Filled bearing_capacity missing values with median: 0.88\n",
      "  üìù Filled plastic_limit_pl missing values with median: 18.90\n",
      "  üìù Filled moisture_content_pct missing values with median: 12.77\n",
      "  üìù Filled bulk_density missing values with median: 1.82\n",
      "  üìù Filled foundation_type missing values with: Unknown\n",
      "‚úÖ Cleaned dataset shape: (4, 22)\n",
      "\n",
      "üìä Step 4: Analysis & Summary\n",
      "\n",
      "============================================================\n",
      "üìä DATASET SUMMARY\n",
      "============================================================\n",
      "\n",
      "üìà Basic Statistics:\n",
      "   Projects: 4\n",
      "   Features: 22\n",
      "   Complete projects (with bearing capacity): 4\n",
      "\n",
      "üéØ Target Variable (Bearing Capacity):\n",
      "   Mean: 0.88 T/ft¬≤\n",
      "   Range: 0.75 - 1.00 T/ft¬≤\n",
      "   Std Dev: 0.10\n",
      "\n",
      "üîç Grain Size Analysis:\n",
      "   ‚úÖ gravel_pct  : 4/4 projects, Range: 0.1% - 6.0%\n",
      "\n",
      "üìã Feature Completeness:\n",
      "   ‚úÖ project_id               : 4/4 (100%)\n",
      "   ‚úÖ bearing_capacity         : 4/4 (100%)\n",
      "   ‚úÖ foundation_type          : 4/4 (100%)\n",
      "   ‚úÖ liquid_limit_ll          : 4/4 (100%)\n",
      "   ‚úÖ depth                    : 4/4 (100%)\n",
      "   ‚úÖ borehole_no              : 4/4 (100%)\n",
      "   ‚úÖ gravel_pct               : 4/4 (100%)\n",
      "   ‚úÖ spt_n_value              : 4/4 (100%)\n",
      "   ‚úÖ plastic_limit_pl         : 4/4 (100%)\n",
      "   ‚úÖ uscs_classification      : 4/4 (100%)\n",
      "   ‚úÖ consistency              : 4/4 (100%)\n",
      "   ‚úÖ soil_color               : 4/4 (100%)\n",
      "   ‚úÖ primary_soil_type        : 4/4 (100%)\n",
      "   ‚úÖ moisture                 : 4/4 (100%)\n",
      "   ‚úÖ depth_start              : 4/4 (100%)\n",
      "   ‚úÖ depth_end                : 4/4 (100%)\n",
      "   ‚úÖ avg_n_value              : 4/4 (100%)\n",
      "   ‚úÖ max_n_value              : 4/4 (100%)\n",
      "   ‚úÖ min_n_value              : 4/4 (100%)\n",
      "   ‚úÖ n_value_count            : 4/4 (100%)\n",
      "   ‚úÖ moisture_content_pct     : 4/4 (100%)\n",
      "   ‚úÖ bulk_density             : 4/4 (100%)\n",
      "\n",
      "üèóÔ∏è Foundation Types:\n",
      "   Unknown: 4\n",
      "\n",
      "üíæ Step 5: Save Results\n",
      "\n",
      "üíæ Dataset saved as: geotechnical_dataset_enhanced_final.csv\n",
      "   Shape: (4, 22)\n",
      "   Columns: ['project_id', 'bearing_capacity', 'foundation_type', 'liquid_limit_ll', 'depth', 'borehole_no', 'gravel_pct', 'spt_n_value', 'plastic_limit_pl', 'uscs_classification', 'consistency', 'soil_color', 'primary_soil_type', 'moisture', 'depth_start', 'depth_end', 'avg_n_value', 'max_n_value', 'min_n_value', 'n_value_count', 'moisture_content_pct', 'bulk_density']\n",
      "\n",
      "‚úÖ Enhanced Extraction Complete!\n",
      "Final dataset: 4 projects √ó 22 features\n",
      "\n",
      "üìã ENHANCED DATASET PREVIEW:\n",
      "  project_id  bearing_capacity foundation_type  liquid_limit_ll         depth   borehole_no  gravel_pct   spt_n_value  plastic_limit_pl uscs_classification consistency soil_color primary_soil_type moisture  depth_start  depth_end  avg_n_value  max_n_value  min_n_value  n_value_count  moisture_content_pct  bulk_density\n",
      "0    7144-25             0.875         Unknown     1.554223e+17  1.040542e+07    437.728571        5.00  2.579500e+02              16.2                  CL        firm        red              sand    moist       7144.0        5.5     8.375000           20            2             16                 12.77      1.824917\n",
      "1    7145-25             0.875         Unknown     1.942779e+17  1.156157e+07  13788.135714        5.00  2.579500e+02              18.9                  CL        firm        red              sand    moist       7145.0        6.0    14.615385           51            2             13                 12.77      1.824917\n",
      "2    7155-25             1.000         Unknown     2.512341e+03  8.718920e+06  14459.832237        0.10  8.446865e+15              18.9                  CL        firm        red              sand      dry       7155.0       10.0    18.387097           64            3             31                  8.70      1.783667\n",
      "3    7157-25             0.750         Unknown     1.195133e+01  1.040546e+07  18063.639033        6.05  4.100000e+01              23.6                  CL        firm        red              sand      dry       7157.0        7.5    19.571429           79            2             35                 16.84      1.866167\n",
      "\n",
      "üîç BEARING CAPACITY VALIDATION:\n",
      "   7144-25: Expected BC=False, Has BC=True (0.875) ‚Üí ‚ùå ERROR\n",
      "   7145-25: Expected BC=False, Has BC=True (0.875) ‚Üí ‚ùå ERROR\n",
      "   7155-25: Expected BC=True, Has BC=True (1.0) ‚Üí ‚úÖ CORRECT\n",
      "   7157-25: Expected BC=True, Has BC=True (0.75) ‚Üí ‚úÖ CORRECT\n",
      "\n",
      "üéØ GRAIN SIZE EXTRACTION RESULTS:\n",
      "   ‚ùå sand_pct    : Column missing\n",
      "   ‚úÖ gravel_pct  : 4/4 projects extracted\n",
      "       Values: [5.0, 5.0, 0.1, 6.05]\n",
      "   ‚ùå fines_pct   : Column missing\n",
      "\n",
      "üéØ READY FOR AI/ML MODELING!\n",
      "Use 'geotechnical_dataset_enhanced_final.csv' for your geotechnical AI system.\n",
      "\n",
      "üìä ENHANCED FEATURES EXTRACTED:\n",
      "    1. ‚úÖ project_id               : 4/4 (100%)\n",
      "    2. ‚úÖ bearing_capacity         : 4/4 (100%)\n",
      "    3. ‚úÖ foundation_type          : 4/4 (100%)\n",
      "    4. ‚úÖ liquid_limit_ll          : 4/4 (100%)\n",
      "    5. ‚úÖ depth                    : 4/4 (100%)\n",
      "    6. ‚úÖ borehole_no              : 4/4 (100%)\n",
      "    7. ‚úÖ gravel_pct               : 4/4 (100%)\n",
      "    8. ‚úÖ spt_n_value              : 4/4 (100%)\n",
      "    9. ‚úÖ plastic_limit_pl         : 4/4 (100%)\n",
      "   10. ‚úÖ uscs_classification      : 4/4 (100%)\n",
      "   11. ‚úÖ consistency              : 4/4 (100%)\n",
      "   12. ‚úÖ soil_color               : 4/4 (100%)\n",
      "   13. ‚úÖ primary_soil_type        : 4/4 (100%)\n",
      "   14. ‚úÖ moisture                 : 4/4 (100%)\n",
      "   15. ‚úÖ depth_start              : 4/4 (100%)\n",
      "   16. ‚úÖ depth_end                : 4/4 (100%)\n",
      "   17. ‚úÖ avg_n_value              : 4/4 (100%)\n",
      "   18. ‚úÖ max_n_value              : 4/4 (100%)\n",
      "   19. ‚úÖ min_n_value              : 4/4 (100%)\n",
      "   20. ‚úÖ n_value_count            : 4/4 (100%)\n",
      "   21. ‚úÖ moisture_content_pct     : 4/4 (100%)\n",
      "   22. ‚úÖ bulk_density             : 4/4 (100%)\n",
      "\n",
      "üéâ ENHANCED PIPELINE COMPLETE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abdulmohiz\\AppData\\Local\\Temp\\ipykernel_21388\\235953042.py:601: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(median_val, inplace=True)\n",
      "C:\\Users\\abdulmohiz\\AppData\\Local\\Temp\\ipykernel_21388\\235953042.py:601: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(median_val, inplace=True)\n",
      "C:\\Users\\abdulmohiz\\AppData\\Local\\Temp\\ipykernel_21388\\235953042.py:601: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(median_val, inplace=True)\n",
      "C:\\Users\\abdulmohiz\\AppData\\Local\\Temp\\ipykernel_21388\\235953042.py:601: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(median_val, inplace=True)\n",
      "C:\\Users\\abdulmohiz\\AppData\\Local\\Temp\\ipykernel_21388\\235953042.py:609: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(mode_val, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: EXECUTE THE ENHANCED PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "# Execute the enhanced geotechnical data extraction pipeline\n",
    "print(\"üöÄ EXECUTING ENHANCED GEOTECHNICAL DATA EXTRACTION PIPELINE\")\n",
    "print(\"=\"*75)\n",
    "\n",
    "final_dataset, summary_stats = main()\n",
    "\n",
    "if final_dataset is not None:\n",
    "    print(\"\\nüìã ENHANCED DATASET PREVIEW:\")\n",
    "    print(final_dataset.head())\n",
    "    \n",
    "    print(\"\\nüîç BEARING CAPACITY VALIDATION:\")\n",
    "    bc_data = final_dataset[['project_id', 'bearing_capacity']].copy()\n",
    "    text_plot_projects = {'7155-25', '7157-25'}  # Known projects with Text Plots\n",
    "    \n",
    "    for idx, row in bc_data.iterrows():\n",
    "        project_id = row['project_id']\n",
    "        bc_value = row['bearing_capacity']\n",
    "        should_have_bc = project_id in text_plot_projects\n",
    "        has_bc = pd.notna(bc_value)\n",
    "        \n",
    "        if should_have_bc and has_bc:\n",
    "            status = \"‚úÖ CORRECT\"\n",
    "        elif not should_have_bc and not has_bc:\n",
    "            status = \"‚úÖ CORRECT\"\n",
    "        else:\n",
    "            status = \"‚ùå ERROR\"\n",
    "        \n",
    "        print(f\"   {project_id}: Expected BC={should_have_bc}, Has BC={has_bc} ({bc_value}) ‚Üí {status}\")\n",
    "    \n",
    "    print(\"\\nüéØ GRAIN SIZE EXTRACTION RESULTS:\")\n",
    "    grain_size_cols = ['sand_pct', 'gravel_pct', 'fines_pct']\n",
    "    for col in grain_size_cols:\n",
    "        if col in final_dataset.columns:\n",
    "            non_null = final_dataset[col].notna().sum()\n",
    "            if non_null > 0:\n",
    "                values = final_dataset[col].dropna()\n",
    "                print(f\"   ‚úÖ {col:<12}: {non_null}/4 projects extracted\")\n",
    "                print(f\"       Values: {list(values)}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå {col:<12}: No data found\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå {col:<12}: Column missing\")\n",
    "    \n",
    "    print(\"\\nüéØ READY FOR AI/ML MODELING!\")\n",
    "    print(\"Use 'geotechnical_dataset_enhanced_final.csv' for your geotechnical AI system.\")\n",
    "    \n",
    "    print(f\"\\nüìä ENHANCED FEATURES EXTRACTED:\")\n",
    "    for i, col in enumerate(final_dataset.columns, 1):\n",
    "        non_null = final_dataset[col].notna().sum()\n",
    "        total = len(final_dataset)\n",
    "        coverage = (non_null/total)*100\n",
    "        status = \"‚úÖ\" if coverage == 100 else \"‚ö†Ô∏è\" if coverage >= 50 else \"‚ùå\"\n",
    "        print(f\"   {i:2d}. {status} {col:<25}: {non_null}/{total} ({coverage:.0f}%)\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Enhanced pipeline execution failed - check your PDF files and data directory\")\n",
    "\n",
    "print(\"\\nüéâ ENHANCED PIPELINE COMPLETE!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a5127c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Debug function defined (uncomment to run)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: DEBUG GRAIN SIZE EXTRACTION (OPTIONAL)\n",
    "# =============================================================================\n",
    "\n",
    "def debug_grain_size_extraction():\n",
    "    \"\"\"Debug function to analyze what's in the PDFs for grain size data\"\"\"\n",
    "    \n",
    "    print(\"üîç DEBUGGING GRAIN SIZE EXTRACTION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    extractor = GeotechnicalDataExtractor(\"Data\")\n",
    "    extractor.identify_file_types()\n",
    "    \n",
    "    # Check first PDF only for debugging\n",
    "    if extractor.complete_reports:\n",
    "        report = extractor.complete_reports[0]\n",
    "        project_id = report['project_id']\n",
    "        file_path = report['file_path']\n",
    "        \n",
    "        print(f\"\\nüìÅ Analyzing Project {project_id}\")\n",
    "        print(f\"   File: {file_path.name}\")\n",
    "        \n",
    "        try:\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                for page_num, page in enumerate(pdf.pages[:3]):  # Check first 3 pages\n",
    "                    text = page.extract_text()\n",
    "                    if not text:\n",
    "                        continue\n",
    "                    \n",
    "                    print(f\"\\nüìÑ Page {page_num + 1}:\")\n",
    "                    \n",
    "                    # Look for grain size related text\n",
    "                    lines = text.split('\\n')\n",
    "                    grain_lines = []\n",
    "                    \n",
    "                    for line in lines:\n",
    "                        if any(keyword in line.lower() for keyword in \n",
    "                               ['sand', 'gravel', 'silt', 'clay', 'sieve', '%', 'grain', 'particle']):\n",
    "                            grain_lines.append(line.strip())\n",
    "                    \n",
    "                    if grain_lines:\n",
    "                        print(\"   üéØ Found grain size related lines:\")\n",
    "                        for line in grain_lines[:10]:  # Show first 10 matches\n",
    "                            print(f\"     {line}\")\n",
    "                    \n",
    "                    # Check tables\n",
    "                    tables = page.extract_tables()\n",
    "                    if tables:\n",
    "                        print(f\"   üìã Found {len(tables)} tables\")\n",
    "                        for table_num, table in enumerate(tables):\n",
    "                            if table and len(table) > 0:\n",
    "                                headers = table[0] if table[0] else []\n",
    "                                print(f\"     Table {table_num + 1} headers: {headers}\")\n",
    "                                \n",
    "                                # Show sample data\n",
    "                                if len(table) > 1:\n",
    "                                    print(f\"     Sample row: {table[1]}\")\n",
    "                    \n",
    "                    if page_num >= 2:  # Only check first 3 pages\n",
    "                        break\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error analyzing {project_id}: {e}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ùå No complete reports found for debugging\")\n",
    "\n",
    "# Uncomment the next line to run debugging\n",
    "# debug_grain_size_extraction()\n",
    "\n",
    "print(\"‚úÖ Debug function defined (uncomment to run)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
